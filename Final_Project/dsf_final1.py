# -*- coding: utf-8 -*-
"""dsf_final1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RimRUDonhOGZt1_8bsqPFb_lLUgI-rWS
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np  
import seaborn as sns 
import statsmodels.api as sm
import matplotlib.pyplot as plt
from pylab import rcParams
import itertools
import os
from sklearn.cluster import KMeans
import warnings 
warnings.filterwarnings("ignore")
import xgboost as xgb
from sklearn.metrics import classification_report,confusion_matrix
from sklearn.svm import SVC
from sklearn.multioutput import MultiOutputClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold, cross_val_score, train_test_split
import keras
from keras.layers import Dense
from keras.models import Sequential
from keras.optimizers import Adam 
from keras.callbacks import EarlyStopping
from keras.utils import np_utils
from keras.layers import LSTM
from sklearn.model_selection import KFold, cross_val_score, train_test_split

full_df=pd.read_csv('/content/drive/My Drive/DataScience/FinalProject/Data/costello.csv')

full_df.columns = [c.replace(' ', '_') for c in full_df.columns]
full_df.columns = [c.replace('-', '_') for c in full_df.columns]

full_df.drop(full_df.loc[full_df['Store_Name'].isna()].index, inplace=True)

def SplitFrame(storeName):
  chunk=full_df[full_df['Store_Name']==storeName]
  chunk.to_csv('/content/drive/My Drive/DataScience/FinalProject/Data/'+storeName+'.csv')

#divide into chunks
lst_OnDivide=full_df['Store_Name'].unique()
for item in lst_OnDivide:
  SplitFrame(item)

storeName = '11116 BELLMORE'

#Reading Chunks
store_df=pd.read_csv('/content/drive/My Drive/DataScience/FinalProject/Data/'+storeName+'.csv', index_col=[0])

store_df_2 = pd.read_csv('/content/drive/My Drive/DataScience/FinalProject/11116 BELLMORE_2.csv', index_col=[0])

store_df = store_df_2.append(store_df)

store_df_2.shape

store_df.shape

#changing data types 
def convertToNum(colname):
  store_df[colname]=store_df[colname].fillna(-999)
  store_df[colname]=store_df[colname].astype(str)
  store_df[colname]=store_df[colname].str.replace(',', '')  
  store_df[colname]=store_df[colname].str.replace('%','')
  store_df[colname]=store_df[colname].str.replace(' ','')
  store_df[colname]=store_df[colname].str.replace('$','')
  store_df[colname]=store_df[colname].str.replace('(','')
  store_df[colname]=store_df[colname].str.replace(')','')
  store_df[colname] = pd.to_numeric(store_df[colname])
def convertToString(colname):
  store_df[colname]=store_df[colname].astype(str)
#Drop rows based on a condition on a column
def dropRows(data_frame,column,conditon):
  data_frame.drop(data_frame.loc[data_frame[column]==conditon].index, inplace=True)
def convertToDateTime(col):
  store_df[col] = pd.to_datetime(store_df[col])

# store_df['Customer_Number']=store_df['Customer_Number'].str.replace('*','')
# #  removing non numeric customers 87 in total ,do this before in store_df 
# store_df=store_df[store_df.Customer_Number.apply(lambda x: x.isnumeric())]
# store_df['Customer_Number'] = pd.to_numeric(store_df['Customer_Number'])

#24 rows with feature values as headers ,so removed using DATE as identifier
store_df.drop(store_df.loc[store_df['Date']=='Date'].index, inplace=True)

#removing duplicated data
store_df.drop_duplicates(keep=False,inplace=True)

#Drop rows where column is NaN in Customer Number
store_df=store_df.dropna(subset=['Customer_Number'])

lstToCOnvert=['Net_Sales_Units','Net_Sales','Cost','Gross_Margin','Gross_Margin_%',
              'Actual_Price','Retail_Price','Item_was_Scanned','Dynamic_Promo_ID','$_Off_Retail','Actual_Retail']
for col in lstToCOnvert:
  convertToNum(col)

convertToDateTime('Date')

########Cleaning
store_df['Class_Code'].fillna('B1',inplace=True)
store_df['Fineline_Code'].fillna('B1',inplace=True)
store_df['MIP_Promo_ID'].fillna('No Discount',inplace=True)
store_df['Promo/Discount'].fillna('No Discount',inplace=True)
store_df['Zip_Code'].fillna('Not Registered ',inplace=True)
store_df['Zip_Plus_4'].fillna('Not Registered ',inplace=True)
store_df['Loyalty_ID'].fillna('Not Registered ',inplace=True)
store_df['Pricing_Source'].fillna('Unknown ',inplace=True)
store_df['Scanned_UPC'].fillna('Unknown ',inplace=True)
store_df['Return_Code'].fillna('Unknown ',inplace=True)
store_df['Item_was_Scanned'].fillna('Unknown ',inplace=True)
store_df.drop(columns='Gross_Margin_%',axis=1,inplace=True)

#Data frame for time series analysis
timeseries_GrossMargin=store_df.filter(['Date','Gross_Margin'])
timeseries_Sales = store_df.filter(['Date','Net_Sales'])

timeseries_GrossMargin = timeseries_GrossMargin.sort_values('Date')
timeseries_GrossMargin.groupby('Date')
timeseries_GrossMargin=timeseries_GrossMargin.set_index('Date')
timeseries_Sales = timeseries_Sales.sort_values('Date')
timeseries_Sales.groupby('Date')
timeseries_Sales=timeseries_Sales.set_index('Date')

def Forecast(period,validate_start,validate_end,predict_start,predict_end):
  if not os.path.isdir('/content/drive/My Drive/DataScience/FinalProject/'+storeName+'___'+period):
    os.makedirs('/content/drive/My Drive/DataScience/FinalProject/'+storeName+'___'+period)
  y_GrossMargin  = timeseries_GrossMargin['Gross_Margin'].resample(period).sum()
  y_Sales  = timeseries_Sales['Net_Sales'].resample(period).sum()
  #Plot Grossmargin compared with Sales
  y_GrossMargin.plot(figsize=(20, 8))
  y_Sales.plot(figsize=(20, 8))
  plt.xlabel("Date")
  plt.ylabel("Total Value")
  plt.title("Gross Margin compared with Sales")
  plt.legend()
  plt.savefig('/content/drive/My Drive/DataScience/FinalProject/'+storeName+'___'+period+'/Gross Margin compared with Sales.png', bbox_inches = "tight")
  plt.show()
  #Seasonal Decompose
  rcParams['figure.figsize'] = 15, 8
  decomposition = sm.tsa.seasonal_decompose(y_GrossMargin, model='additive')  
  fig = decomposition.plot()
  plt.savefig('/content/drive/My Drive/DataScience/FinalProject/'+storeName+'___'+period+'/decomposition.png', bbox_inches = "tight")
  plt.show()
  #Sarimax  model fitting
  p = d = q = range(0, 2)
  pdq = list(itertools.product(p, d, q))
  seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]
  #split data into training data
  y_GrossMargin_train=y_GrossMargin['2015-01':'2018-06']
  ##############################
  for param in pdq:
      for param_seasonal in seasonal_pdq:
          try:
              mod = sm.tsa.statespace.SARIMAX(y_GrossMargin_train,
                                              order=param,
                                              seasonal_order=param_seasonal,
                                              enforce_stationarity=False,
                                              enforce_invertibility=False)
              results = mod.fit()
              print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))
          except:
              continue
  # Optimal values of p,d and q ->ARIMA(0, 0, 1)x(0, 1, 0, 12)12 - AIC:26.099158099099753
  #ARIMA(0, 1, 1)x(0, 1, 0, 12)12 - AIC:231.22919432787555
  mod = sm.tsa.statespace.SARIMAX(y_GrossMargin_train,
                                  order=(0, 1, 1),
                                  seasonal_order=(0, 1, 0, 12),
                                  enforce_stationarity=False,
                                  enforce_invertibility=False)
  results = mod.fit()
  print(results.summary())
  #Validate model 
  #Used  data from 2018 onwards to validate the model by comparing the observed and the predicted
  pred = results.get_prediction(start=pd.to_datetime(validate_start),end= pd.to_datetime(validate_end), dynamic=False)
  pred_ci = pred.conf_int()
  ax = y_GrossMargin['2015':].plot(label='observed')
  pred.predicted_mean.plot(ax=ax, label='Validate', alpha=.7, figsize=(14, 7),linewidth=3)
  ax.fill_between(pred_ci.index,
                  pred_ci.iloc[:, 0],
                  pred_ci.iloc[:, 1], color='k', alpha=.1)
  ax.set_xlabel('Date')
  ax.set_ylabel('Gross Margin')
  plt.legend()
  plt.savefig('/content/drive/My Drive/DataScience/FinalProject/'+storeName+'___'+period+'/validate.png', bbox_inches = "tight")
  plt.show()
  #Validation error handling
  y_forecasted = pred.predicted_mean
  y_truth = y_GrossMargin['2018-01-01':]
  mse = ((y_forecasted - y_truth) ** 2).mean()
  print('The Mean Squared Error of our validation is {}'.format(round(mse, 2)))
  print('The Root Mean Squared Error of our validation is {}'.format(round(np.sqrt(mse), 2)))

  #forecasting or predicting for future and connecting it to previous ggraph
  pred = results.get_prediction(start=pd.to_datetime(predict_start),end=predict_end, dynamic=False)
  pred_ci = pred.conf_int()
  ax = y_GrossMargin['2015':].plot(label='observed')
  pred.predicted_mean.plot(ax=ax, label='Forecast', alpha=.7, figsize=(14, 7))
  ax.fill_between(pred_ci.index,
                  pred_ci.iloc[:, 0],
                  pred_ci.iloc[:, 1], color='k', alpha=.2)
  ax.set_xlabel('Date')
  ax.set_ylabel('Gross Margin')
  plt.legend()
  plt.savefig('/content/drive/My Drive/DataScience/FinalProject/'+storeName+'___'+period+'/Forecast.png', bbox_inches = "tight")
  plt.show()

#Validate Monthly
validate_start='2018-05-01'
validate_end='2018-12-01'
#Predict Monthly
predict_start='2018-12-01'
predict_end='2020-01-01'
Forecast('MS',validate_start,validate_end,predict_start,predict_end)

#Validate weekly
validate_start='2018-06-03'
validate_end='2018-12-30'
#Predict weekly
predict_start='2018-12-30'
predict_end='2020-01-05'
Forecast('W',validate_start,validate_end,predict_start,predict_end)

if not os.path.isdir('/content/drive/My Drive/DataScience/FinalProject/'+storeName):
    os.makedirs('/content/drive/My Drive/DataScience/FinalProject/'+storeName)
new_df=store_df.copy()
new_df['Transaction_Time']=pd.to_datetime(new_df['Transaction_Time'])
new_df['hour'] = new_df['Transaction_Time'].dt.hour
df_peakTransactions=new_df.groupby(['hour'])
peaktimings = df_peakTransactions.Receipt_Number.nunique().reset_index()
plt.plot(peaktimings.hour,peaktimings.Receipt_Number,color='r')
plt.xticks(range(24))
plt.xlabel('Time of the day')
plt.ylabel('Number of transactions')
plt.title('Peak Transactions by time of the day')
plt.savefig('/content/drive/My Drive/DataScience/FinalProject/'+storeName+'/PeakTransactionsbyTime.png', bbox_inches = "tight")
plt.show()

store_df.groupby(['Date','Receipt_Number']).count()

df=store_df.groupby('Item_Description')['Date'].count().sort_values(ascending=False).reset_index()
top_df=df.iloc[:5]
plt.bar(top_df.Item_Description,top_df.Date,color='c')
plt.xlabel('Products')
plt.ylabel('Count')
plt.title("Type Returned the maximum number of times")
plt.savefig('/content/drive/My Drive/DataScience/FinalProject/'+storeName+'/ProductsReturnedMaximum.png', bbox_inches = "tight")
plt.show()

os.makedirs('/content/drive/My Drive/DataScience/FinalProject/'+storeName+'___'+)

#####################Product time series####################
def productForecast(product,df,period,validate_start,validate_end,predict_start,predict_end):
  if not os.path.isdir('/content/drive/My Drive/DataScience/FinalProject/'+storeName+'___'+product):
    os.makedirs('/content/drive/My Drive/DataScience/FinalProject/'+storeName+'___'+product)
  y_NetSalesUnits  = df.Net_Sales_Units.resample(period).sum()
  #Plot Grossmargin compared with Sales
  y_NetSalesUnits.plot(figsize=(20, 8))
  plt.xlabel("Date")
  plt.ylabel("Total Units")
  plt.title("Sales Units time series")
  plt.legend()
  plt.savefig('/content/drive/My Drive/DataScience/FinalProject/'+storeName+'___'+product+'/Sales Units time Series.png', bbox_inches = "tight")
  plt.show()
  #Seasonal Decompose
  rcParams['figure.figsize'] = 15, 8
  decomposition = sm.tsa.seasonal_decompose(y_NetSalesUnits, model='additive')  
  fig = decomposition.plot()
  plt.savefig('/content/drive/My Drive/DataScience/FinalProject/'+storeName+'___'+product+'/decomposition.png', bbox_inches = "tight")
  plt.show()
  #Sarimax  model fitting
  p = d = q = range(0, 2)
  pdq = list(itertools.product(p, d, q))
  seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]
  #split data into training data
  y_NetSalesUnits_train=y_NetSalesUnits['2015-01-01':'2018-06']
  ##############################
  for param in pdq:
      for param_seasonal in seasonal_pdq:
          try:
              mod = sm.tsa.statespace.SARIMAX(y_NetSalesUnits_train,
                                              order=param,
                                              seasonal_order=param_seasonal,
                                              enforce_stationarity=False,
                                              enforce_invertibility=False)
              results = mod.fit()
              print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))
          except:
              continue
  # Optimal values of p,d and q ->ARIMA(0, 0, 1)x(0, 1, 0, 12)12 - AIC:26.099158099099753
  #ARIMA(0, 1, 1)x(0, 1, 0, 12)12 - AIC:231.22919432787555
  mod = sm.tsa.statespace.SARIMAX(y_NetSalesUnits_train,
                                  order=(0, 1, 1),
                                  seasonal_order=(0, 1, 0, 12),
                                  enforce_stationarity=False,
                                  enforce_invertibility=False)
  results = mod.fit()
  print(results.summary())
  ##Plot Diagnostics
  #error
  #results.plot_diagnostics(figsize=(16, 8))
  #plt.savefig('/content/drive/My Drive/DataScience/FinalProject/'+storeName+'___'+product+'/diagnostics.png', bbox_inches = "tight")
  #plt.show()

  #Validate model 
  #Used  data from 2018 onwards to validate the model by comparing the observed and the predicted
  pred = results.get_prediction(start=pd.to_datetime(validate_start),end= pd.to_datetime(validate_end), dynamic=False)
  pred_ci = pred.conf_int()
  ax = y_NetSalesUnits['2015':].plot(label='observed')
  pred.predicted_mean.plot(ax=ax, label='Validate', alpha=.7, figsize=(14, 7),linewidth=3)
  ax.fill_between(pred_ci.index,
                  pred_ci.iloc[:, 0],
                  pred_ci.iloc[:, 1], color='k', alpha=.1)
  ax.set_xlabel('Date')
  ax.set_ylabel('Net Sales Units')
  plt.title("Validation model for Net Sales Unit for the Product")
  plt.legend()
  plt.savefig('/content/drive/My Drive/DataScience/FinalProject/'+storeName+'___'+product+'/validate.png', bbox_inches = "tight")
  plt.show()
  #Validation error handling
  y_forecasted = pred.predicted_mean
  y_truth = y_NetSalesUnits['2018-01-01':]
  mse = ((y_forecasted - y_truth) ** 2).mean()
  print('The Mean Squared Error of our validation is {}'.format(round(mse, 2)))
  print('The Root Mean Squared Error of our validation is {}'.format(round(np.sqrt(mse), 2)))

  #forecasting or predicting for future and connecting it to previous ggraph
  pred = results.get_prediction(start=pd.to_datetime(predict_start),end=predict_end, dynamic=False)
  pred_ci = pred.conf_int()
  ax = y_NetSalesUnits['2015':].plot(label='observed')
  pred.predicted_mean.plot(ax=ax, label='Forecasted Stocks', alpha=.7, figsize=(14, 7))
  ax.fill_between(pred_ci.index,
                  pred_ci.iloc[:, 0],
                  pred_ci.iloc[:, 1], color='k', alpha=.2)
  ax.set_xlabel('Date')
  ax.set_ylabel('Stocks')
  plt.title("Forecasted Stocks for the Product")
  plt.legend()
  plt.savefig('/content/drive/My Drive/DataScience/FinalProject/'+storeName+'___'+product+'/Forecast.png', bbox_inches = "tight")
  plt.show()

#selecting top 3 net sale units products 
productTimeSeries_df = store_df.filter(['Date','Item_Description','Net_Sales_Units'])
topProducts = productTimeSeries_df.groupby('Item_Description').Net_Sales_Units.sum().sort_values(ascending=False).index[:4].tolist()
topProducts = topProducts[:2]+[topProducts[-1]]
topProducts

for product in topProducts:
  df = productTimeSeries_df[productTimeSeries_df['Item_Description']==product]
  df = df.filter(['Date','Net_Sales_Units'])
  df = df.sort_values('Date')
  df.groupby('Date')
  df=df.set_index('Date')
  #validate for product monthly
  validate_start='2018-05-01'
  validate_end='2018-12-01'
  #predict for product monthly
  predict_start='2018-12-01'
  predict_end='2020-01-01'
  productForecast(product,df,'MS',validate_start,validate_end,predict_start,predict_end)

########Cleaning##############
##########new report start
#Drop *5 customer number rows
store_df.drop(store_df[store_df['Customer_Number'].str.startswith('*5')].index,inplace=True)

#Data frame for recency score mapped with each customer number
customer_recency = pd.DataFrame(store_df['Customer_Number'].unique())
customer_recency.columns=['Customer_Number']

#Get latest date for each customer transaction
latest_purchase_date = store_df.groupby('Customer_Number').Date.max().reset_index()
latest_purchase_date.columns = ['Customer_Number','Latest_purchase_date']

#Recency score calculation
latest_purchase_date['Recency'] = (latest_purchase_date['Latest_purchase_date'].max() - latest_purchase_date['Latest_purchase_date']).dt.days
#merge this dataframe to customer dataframe on cutsomer_number
customer_recency = pd.merge(customer_recency, latest_purchase_date[['Customer_Number','Recency']], on='Customer_Number')

customer_recency.hist(bins=80)
plt.savefig('/content/drive/My Drive/DataScience/FinalProject/'+storeName+'/Recency.png', bbox_inches = "tight")

#Finding optimal clusters for KMeans based on Recency,Frequency,Monetary Value
def OptimalClusters(parameter):
  sse={}
  parameter = customer_recency[[parameter]]
  for k in range(1, 10):
      kmeans = KMeans(n_clusters=k, max_iter=1000).fit(parameter)
      parameter["clusters"] = kmeans.labels_
      sse[k] = kmeans.inertia_ 
  plt.figure(figsize=(15,8))
  plt.plot(list(sse.keys()), list(sse.values()))
  plt.xlabel("Number of cluster")
  plt.title('Inertia Graph')
  plt.savefig('/content/drive/My Drive/DataScience/FinalProject/'+storeName+'/Inertia_Graph.png', bbox_inches = "tight")
  plt.show()

#Optimal clusters for recency
OptimalClusters('Recency')

#build 4 clusters for recency and add it to dataframe
def Initialize(parameter):
  kmeans = KMeans(n_clusters=4)
  kmeans.fit(customer_recency[[parameter]])
  new_parameter_name=parameter+'Cluster'
  customer_recency[new_parameter_name] = kmeans.predict(customer_recency[[parameter]])

#function for ordering cluster numbers
def order_cluster(cluster_field_name, target_field_name,df,ascending):
    new_cluster_field_name = 'new_' + cluster_field_name
    df_new = df.groupby(cluster_field_name)[target_field_name].mean().reset_index()
    df_new = df_new.sort_values(by=target_field_name,ascending=ascending).reset_index(drop=True)
    df_new['index'] = df_new.index
    df_final = pd.merge(df,df_new[[cluster_field_name,'index']], on=cluster_field_name)
    df_final = df_final.drop([cluster_field_name],axis=1)
    df_final = df_final.rename(columns={"index":cluster_field_name})
    return df_final
Initialize('Recency')
customer_recency = order_cluster('RecencyCluster', 'Recency',customer_recency,False)

#In above function we got customers clustered based  on recency score,where 3rd cluster is the most recent. 
customer_recency[customer_recency['RecencyCluster']==3]
customer_recency.groupby('RecencyCluster')['Recency'].describe()

#Customer Frequency calculation
customer_frequency = store_df.groupby('Customer_Number').Date.count().reset_index()
customer_frequency.columns = ['Customer_Number','Frequency']

#adding this data to our main dataframe
customer_recency = pd.merge(customer_recency, customer_frequency, on='Customer_Number')

plt.hist(x=customer_recency.query('Frequency < 5000')['Frequency'],bins=500)
plt.xlim(0,300)
plt.title("Frequency")
plt.savefig('/content/drive/My Drive/DataScience/FinalProject/'+storeName+'/Frequency.png', bbox_inches = "tight")

Initialize('Frequency')
customer_recency = order_cluster('FrequencyCluster', 'Frequency',customer_recency,True)

#high frequency number means better customers.
customer_recency.groupby('FrequencyCluster')['Frequency'].describe()

#calculate revenue for each customer
customer_revenue = store_df.groupby('Customer_Number')['Gross_Margin'].sum().reset_index()

#merge it with our main dataframe
customer_recency = pd.merge(customer_recency, customer_revenue, on='Customer_Number')

plt.hist(x=customer_recency['Gross_Margin'],bins=20000)
plt.xlim(-1000,4000)
plt.savefig('/content/drive/My Drive/DataScience/FinalProject/'+storeName+'/Revenue.png', bbox_inches = "tight")

Initialize('Gross_Margin')
customer_recency = order_cluster('Gross_MarginCluster', 'Gross_Margin',customer_recency,True)
customer_recency.groupby('Gross_MarginCluster')['Gross_Margin'].describe()

#calculate RFM score  
customer_recency['RFMScore'] = customer_recency['RecencyCluster'] + customer_recency['FrequencyCluster'] + customer_recency['Gross_MarginCluster']
customer_recency.groupby('RFMScore')['Recency','Frequency','Gross_Margin'].mean()

#Divide cutomers based on the score
customer_recency['Segment'] = 'Low-Value'
customer_recency.loc[customer_recency['RFMScore']>2,'Segment'] = 'Mid-Value' 
customer_recency.loc[customer_recency['RFMScore']>4,'Segment'] = 'High-Value'

import plotly as py
import plotly.offline as pyoff
import plotly.graph_objs as go
import plotly.io as pio
pyoff.init_notebook_mode(connected=True)

# import plotly.io as pio

pio.renderers.default = "colab"
pio.renderers

def configure_plotly_browser_state():
  import IPython
  display(IPython.core.display.HTML('''
        <script src="/static/components/requirejs/require.js"></script>
        <script>
          requirejs.config({
            paths: {
              base: '/static/base',
              plotly: 'https://cdn.plot.ly/plotly-latest.min.js?noext',
            },
          });
        </script>
        '''))

configure_plotly_browser_state()
def PlotGraph_scatter(feature1,feature2):
#Revenue vs Frequency plots 
  customer_recency_graph = customer_recency.query("Gross_Margin < 2000000 and Frequency < 400000")
  plot_data = [
      go.Scatter(
          x=customer_recency_graph.query("Segment == 'Low-Value'")[feature1],
          y=customer_recency_graph.query("Segment == 'Low-Value'")[feature2],
          mode='markers',
          name='Low',
          marker= dict(size= 7,
              line= dict(width=1),
              color= 'blue',
              opacity= 0.8
            )
      ),
          go.Scatter(
          x=customer_recency_graph.query("Segment == 'Mid-Value'")[feature1],
          y=customer_recency_graph.query("Segment == 'Mid-Value'")[feature2],
          mode='markers',
          name='Mid',
          marker= dict(size= 9,
              line= dict(width=1),
              color= 'green',
              opacity= 0.5
            )
      ),
          go.Scatter(
          x=customer_recency_graph.query("Segment == 'High-Value'")[feature1],
          y=customer_recency_graph.query("Segment == 'High-Value'")[feature2],
          mode='markers',
          name='High',
          marker= dict(size= 11,
              line= dict(width=1),
              color= 'red',
              opacity= 0.9
            )
      ),
  ]

  plot_layout = go.Layout(
          yaxis= {'title': feature2},
          xaxis= {'title': feature1},
          title='Segments'
      )
  fig = go.Figure(data=plot_data, layout=plot_layout)
  pyoff.iplot(fig)
PlotGraph_scatter('Frequency','Gross_Margin')

#Revenue Recency
tx_graph = customer_recency.query("Gross_Margin < 2000000 and Frequency < 400000")
# tx_graph = customer_recency
plot_data = [
    go.Scatter(
        x=tx_graph.query("Segment == 'Low-Value'")['Recency'],
        y=tx_graph.query("Segment == 'Low-Value'")['Gross_Margin'],
        mode='markers',
        name='Low',
        marker= dict(size= 7,
            line= dict(width=1),
            color= 'blue',
            opacity= 0.8
           )
    ),
        go.Scatter(
        x=tx_graph.query("Segment == 'Mid-Value'")['Recency'],
        y=tx_graph.query("Segment == 'Mid-Value'")['Gross_Margin'],
        mode='markers',
        name='Mid',
        marker= dict(size= 9,
            line= dict(width=1),
            color= 'green',
            opacity= 0.5
           )
    ),
        go.Scatter(
        x=tx_graph.query("Segment == 'High-Value'")['Recency'],
        y=tx_graph.query("Segment == 'High-Value'")['Gross_Margin'],
        mode='markers',
        name='High',
        marker= dict(size= 11,
            line= dict(width=1),
            color= 'red',
            opacity= 0.9
           )
    ),
]

plot_layout = go.Layout(
        yaxis= {'title': "Gross_Margin"},
        xaxis= {'title': "Recency"},
        title='Segments'
    )
fig = go.Figure(data=plot_data, layout=plot_layout)
pyoff.iplot(fig)

# Revenue vs Frequency
tx_graph = customer_recency.query("Gross_Margin < 2000000 and Frequency < 400000")
# tx_graph = customer_recency
plot_data = [
    go.Scatter(
        x=tx_graph.query("Segment == 'Low-Value'")['Recency'],
        y=tx_graph.query("Segment == 'Low-Value'")['Frequency'],
        mode='markers',
        name='Low',
        marker= dict(size= 7,
            line= dict(width=1),
            color= 'blue',
            opacity= 0.8
           )
    ),
        go.Scatter(
        x=tx_graph.query("Segment == 'Mid-Value'")['Recency'],
        y=tx_graph.query("Segment == 'Mid-Value'")['Frequency'],
        mode='markers',
        name='Mid',
        marker= dict(size= 9,
            line= dict(width=1),
            color= 'green',
            opacity= 0.5
           )
    ),
        go.Scatter(
        x=tx_graph.query("Segment == 'High-Value'")['Recency'],
        y=tx_graph.query("Segment == 'High-Value'")['Frequency'],
        mode='markers',
        name='High',
        marker= dict(size= 11,
            line= dict(width=1),
            color= 'red',
            opacity= 0.9
           )
    ),
]

plot_layout = go.Layout(
        yaxis= {'title': "Frequency"},
        xaxis= {'title': "Recency"},
        title='Segments'
    )
fig = go.Figure(data=plot_data, layout=plot_layout)
pyoff.iplot(fig)

###New Topic ->>>>>>>>>>>>>>>>>>>Life time value  
import datetime 
#create 3m and 6m dataframes
tx_3m = store_df[(store_df.Date > datetime.date(2017,1,1)) & (store_df.Date <= datetime.date(2018,6,1))].reset_index(drop=True)   #training
tx_6m = store_df[(store_df.Date > datetime.date(2018,6,1)) & (store_df.Date <= datetime.date(2018,12,31))].reset_index(drop=True) #testing

#create tx_user for assigning clustering
tx_user = pd.DataFrame(tx_3m['Customer_Number'].unique())
tx_user.columns = ['Customer_Number']

#calculate recency score
tx_max_purchase = tx_3m.groupby('Customer_Number').Date.max().reset_index()
tx_max_purchase.columns = ['Customer_Number','MaxPurchaseDate']
tx_max_purchase['Recency'] = (tx_max_purchase['MaxPurchaseDate'].max() - tx_max_purchase['MaxPurchaseDate']).dt.days
tx_user = pd.merge(tx_user, tx_max_purchase[['Customer_Number','Recency']], on='Customer_Number')

kmeans = KMeans(n_clusters=4)
kmeans.fit(tx_user[['Recency']])
tx_user['RecencyCluster'] = kmeans.predict(tx_user[['Recency']])
tx_user = order_cluster('RecencyCluster', 'Recency',tx_user,False)

#calcuate frequency score
tx_frequency = tx_3m.groupby('Customer_Number').Date.count().reset_index()
tx_frequency.columns = ['Customer_Number','Frequency']
tx_user = pd.merge(tx_user, tx_frequency, on='Customer_Number')

kmeans = KMeans(n_clusters=4)
kmeans.fit(tx_user[['Frequency']])
tx_user['FrequencyCluster'] = kmeans.predict(tx_user[['Frequency']])

tx_user = order_cluster('FrequencyCluster', 'Frequency',tx_user,True)

#calcuate revenue score
#tx_3m['Revenue'] = tx_3m['UnitPrice'] * tx_3m['Quantity']
tx_revenue = tx_3m.groupby('Customer_Number')['Gross_Margin'].sum().reset_index()
tx_user = pd.merge(tx_user, tx_revenue, on='Customer_Number')

kmeans = KMeans(n_clusters=4)
kmeans.fit(tx_user[['Gross_Margin']])
tx_user['RevenueCluster'] = kmeans.predict(tx_user[['Gross_Margin']])
tx_user = order_cluster('RevenueCluster', 'Gross_Margin',tx_user,True)

#overall scoring
tx_user['OverallScore'] = tx_user['RecencyCluster'] + tx_user['FrequencyCluster'] + tx_user['RevenueCluster']
tx_user['Segment'] = 'Low-Value'
tx_user.loc[tx_user['OverallScore']>2,'Segment'] = 'Mid-Value' 
tx_user.loc[tx_user['OverallScore']>4,'Segment'] = 'High-Value'

#calculate revenue and create a new dataframe for it
tx_user_6m = tx_6m.groupby('Customer_Number')['Gross_Margin'].sum().reset_index()
tx_user_6m.columns = ['Customer_Number','m6_Revenue']

tx_user_6m['m6_Revenue'].head()

#plot LTV histogram
plt.hist(x=tx_user_6m['m6_Revenue'],bins=20000)
plt.xlim(-500,1500)
plt.title("Life Time Value of Customers tested on 6 Month data")
plt.savefig('/content/drive/My Drive/DataScience/FinalProject/'+storeName+'/LTV_6mRevenue.png', bbox_inches = "tight")

tx_merge = pd.merge(tx_user, tx_user_6m, on='Customer_Number', how='left')
tx_merge = tx_merge.fillna(0)

tx_graph = tx_merge.query("m6_Revenue < 300000")
plot_data = [
    go.Scatter(
        x=tx_graph.query("Segment == 'Low-Value'")['OverallScore'],
        y=tx_graph.query("Segment == 'Low-Value'")['m6_Revenue'],
        mode='markers',
        name='Low',
        marker= dict(size= 7,
            line= dict(width=1),
            color= 'blue',
            opacity= 0.8
           )
    ),
        go.Scatter(
        x=tx_graph.query("Segment == 'Mid-Value'")['OverallScore'],
        y=tx_graph.query("Segment == 'Mid-Value'")['m6_Revenue'],
        mode='markers',
        name='Mid',
        marker= dict(size= 9,
            line= dict(width=1),
            color= 'green',
            opacity= 0.5
           )
    ),
        go.Scatter(
        x=tx_graph.query("Segment == 'High-Value'")['OverallScore'],
        y=tx_graph.query("Segment == 'High-Value'")['m6_Revenue'],
        mode='markers',
        name='High',
        marker= dict(size= 11,
            line= dict(width=1),
            color= 'red',
            opacity= 0.9
           )
    ),
]

plot_layout = go.Layout(
        yaxis= {'title': "6m LTV"},
        xaxis= {'title': "RFM Score"},
        title='LTV'
    )
fig = go.Figure(data=plot_data, layout=plot_layout)
pyoff.iplot(fig)

#remove outliers
tx_merge = tx_merge[tx_merge['m6_Revenue']<tx_merge['m6_Revenue'].quantile(0.99)]
tx_merge = tx_merge[tx_merge['m6_Revenue']>tx_merge['m6_Revenue'].quantile(0.01)]


#creating 3 clusters
kmeans = KMeans(n_clusters=3)
kmeans.fit(tx_merge[['m6_Revenue']])
tx_merge['LTVCluster'] = kmeans.predict(tx_merge[['m6_Revenue']])

#order cluster number based on LTV
tx_merge = order_cluster('LTVCluster', 'm6_Revenue',tx_merge,True)

#creating a new cluster dataframe
tx_cluster = tx_merge.copy()

#see details of the clusters
tx_cluster.groupby('LTVCluster')['m6_Revenue'].describe()

#convert categorical columns to numerical
tx_class  = pd.get_dummies(tx_cluster, columns=['Segment'])
# df.head()

#calculate and show correlations
corr_matrix = tx_class.corr()
corr_matrix['LTVCluster'].sort_values(ascending=False)

########Cleaning##############
tx_class['Customer_Number']=tx_class['Customer_Number'].str.replace('*','')

#  removing non numeric customers 87 in total ,do this before in store_df 
tx_class=tx_class[tx_class.Customer_Number.apply(lambda x: x.isnumeric())]

tx_class['Customer_Number'] = pd.to_numeric(tx_class['Customer_Number'])

#create X and y, X will be feature set and y is the label - LTV
X = tx_class.drop(['LTVCluster','m6_Revenue'],axis=1)
y = tx_class['LTVCluster']

#split training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=56)

#XGBoost Multiclassification Model
ltv_xgb_model = xgb.XGBClassifier(max_depth=5, learning_rate=0.1,objective= 'multi:softprob',n_jobs=-1).fit(X_train, y_train)

print('Accuracy of XGB classifier on training set: {:.2f}'
       .format(ltv_xgb_model.score(X_train, y_train)))
print('Accuracy of XGB classifier on test set: {:.2f}'
       .format(ltv_xgb_model.score(X_test[X_train.columns], y_test)))

y_pred = ltv_xgb_model.predict(X_test)
print(classification_report(y_test, y_pred))

#######Predicting next purchase day##################
store_df.head()

########Cleaning##############
store_df['Customer_Number']=store_df['Customer_Number'].str.replace('*','')
#  removing non numeric customers 87 in total ,do this before in store_df 
store_df=store_df[store_df.Customer_Number.apply(lambda x: x.isnumeric())]
store_df['Customer_Number'] = pd.to_numeric(store_df['Customer_Number'])

tx_6m = store_df[(store_df.Date > datetime.date(2017,1,1)) & (store_df.Date <= datetime.date(2018,6,1))].reset_index(drop=True)
tx_next = store_df[(store_df.Date > datetime.date(2018,6,1)) & (store_df.Date <= datetime.date(2018,12,31))].reset_index(drop=True)

tx_user = pd.DataFrame(tx_6m['Customer_Number'].unique())
tx_user.columns = ['Customer_Number']

#create a dataframe with customer id and first purchase date in tx_next
tx_next_first_purchase = tx_next.groupby('Customer_Number').Date.min().reset_index()
tx_next_first_purchase.columns = ['Customer_Number','MinPurchaseDate']

#create a dataframe with customer id and last purchase date in tx_6m
tx_last_purchase = tx_6m.groupby('Customer_Number').Date.max().reset_index()
tx_last_purchase.columns = ['Customer_Number','MaxPurchaseDate']

#merge two dataframes
tx_purchase_dates = pd.merge(tx_last_purchase,tx_next_first_purchase,on='Customer_Number',how='left')

#calculate the time difference in days:
tx_purchase_dates['NextPurchaseDay'] = (tx_purchase_dates['MinPurchaseDate'] - tx_purchase_dates['MaxPurchaseDate']).dt.days

#merge with tx_user 
tx_user = pd.merge(tx_user, tx_purchase_dates[['Customer_Number','NextPurchaseDay']],on='Customer_Number',how='left')

#print tx_user
tx_user.head()

#fill NA values with 999
tx_user = tx_user.fillna(999)

tx_user[tx_user.NextPurchaseDay==999]

tx_user.head()

#get max purchase date for Recency and create a dataframe
tx_max_purchase = tx_6m.groupby('Customer_Number').Date.max().reset_index()
tx_max_purchase.columns = ['Customer_Number','MaxPurchaseDate']

#find the recency in days and add it to tx_user
tx_max_purchase['Recency'] = (tx_max_purchase['MaxPurchaseDate'].max() - tx_max_purchase['MaxPurchaseDate']).dt.days
tx_user = pd.merge(tx_user, tx_max_purchase[['Customer_Number','Recency']], on='Customer_Number')

#plot recency
plot_data = [
    go.Histogram(
        x=tx_user['Recency']
    )
]

plot_layout = go.Layout(
        title='Recency'
    )
fig = go.Figure(data=plot_data, layout=plot_layout)
pyoff.iplot(fig)

#clustering for Recency
kmeans = KMeans(n_clusters=4)
kmeans.fit(tx_user[['Recency']])
tx_user['RecencyCluster'] = kmeans.predict(tx_user[['Recency']])

#order recency clusters
tx_user = order_cluster('RecencyCluster', 'Recency',tx_user,False)

#print cluster characteristics
tx_user.groupby('RecencyCluster')['Recency'].describe()


#get total purchases for frequency scores
tx_frequency = tx_6m.groupby('Customer_Number').Date.count().reset_index()
tx_frequency.columns = ['Customer_Number','Frequency']

#add frequency column to tx_user
tx_user = pd.merge(tx_user, tx_frequency, on='Customer_Number')

tx_user['Frequency'].describe()

#plot frequency
plot_data = [
    go.Histogram(
        x=tx_user.query('Frequency < 2000')['Frequency']
    )
]

plot_layout = go.Layout(
        title='Frequency'
    )
fig = go.Figure(data=plot_data, layout=plot_layout)
pyoff.iplot(fig)

#clustering for frequency
kmeans = KMeans(n_clusters=4)
kmeans.fit(tx_user[['Frequency']])
tx_user['FrequencyCluster'] = kmeans.predict(tx_user[['Frequency']])

#order frequency clusters and show the characteristics
tx_user = order_cluster('FrequencyCluster', 'Frequency',tx_user,True)
tx_user.groupby('FrequencyCluster')['Frequency'].describe()


#calculate monetary value, create a dataframe with it
#tx_6m['Revenue'] = tx_6m['UnitPrice'] * tx_6m['Quantity']
tx_revenue = tx_6m.groupby('Customer_Number').Gross_Margin.sum().reset_index()

#add Revenue column to tx_user
tx_user = pd.merge(tx_user, tx_revenue, on='Customer_Number')

#plot Revenue
plot_data = [
    go.Histogram(
        x=tx_user.query('Gross_Margin < 10000')['Gross_Margin']
    )
]

plot_layout = go.Layout(
        title='Monetary Value'
    )
fig = go.Figure(data=plot_data, layout=plot_layout)
pyoff.iplot(fig)

#Revenue clusters 
kmeans = KMeans(n_clusters=4)
kmeans.fit(tx_user[['Gross_Margin']])
tx_user['RevenueCluster'] = kmeans.predict(tx_user[['Gross_Margin']])

#ordering clusters and who the characteristics
tx_user = order_cluster('RevenueCluster', 'Gross_Margin',tx_user,True)
tx_user.groupby('RevenueCluster')['Gross_Margin'].describe()
#building overall segmentation
tx_user['OverallScore'] = tx_user['RecencyCluster'] + tx_user['FrequencyCluster'] + tx_user['RevenueCluster']

#assign segment names
tx_user['Segment'] = 'Low-Value'
tx_user.loc[tx_user['OverallScore']>2,'Segment'] = 'Mid-Value' 
tx_user.loc[tx_user['OverallScore']>4,'Segment'] = 'High-Value'

tx_user.Frequency.describe()

#plot revenue vs frequency
tx_graph = tx_user.query("Gross_Margin < 50000 and Frequency < 1500")
#tx_graph=tx_user.copy()
plot_data = [
    go.Scatter(
        x=tx_graph.query("Segment == 'Low-Value'")['Frequency'],
        y=tx_graph.query("Segment == 'Low-Value'")['Gross_Margin'],
        mode='markers',
        name='Low',
        marker= dict(size= 7,
            line= dict(width=1),
            color= 'blue',
            opacity= 0.8
           )
    ),
        go.Scatter(
        x=tx_graph.query("Segment == 'Mid-Value'")['Frequency'],
        y=tx_graph.query("Segment == 'Mid-Value'")['Gross_Margin'],
        mode='markers',
        name='Mid',
        marker= dict(size= 9,
            line= dict(width=1),
            color= 'green',
            opacity= 0.5
           )
    ),
        go.Scatter(
        x=tx_graph.query("Segment == 'High-Value'")['Frequency'],
        y=tx_graph.query("Segment == 'High-Value'")['Gross_Margin'],
        mode='markers',
        name='High',
        marker= dict(size= 11,
            line= dict(width=1),
            color= 'red',
            opacity= 0.9
           )
    ),
]

plot_layout = go.Layout(
        yaxis= {'title': "Gross_Margin"},
        xaxis= {'title': "Frequency"},
        title='Segments'
    )
fig = go.Figure(data=plot_data, layout=plot_layout)
pyoff.iplot(fig)

#plot revenue vs recency
tx_graph = tx_user.query("Gross_Margin < 50000 and Frequency < 2000")
#tx_graph=tx_user.copy()
plot_data = [
    go.Scatter(
        x=tx_graph.query("Segment == 'Low-Value'")['Recency'],
        y=tx_graph.query("Segment == 'Low-Value'")['Gross_Margin'],
        mode='markers',
        name='Low',
        marker= dict(size= 7,
            line= dict(width=1),
            color= 'blue',
            opacity= 0.8
           )
    ),
        go.Scatter(
        x=tx_graph.query("Segment == 'Mid-Value'")['Recency'],
        y=tx_graph.query("Segment == 'Mid-Value'")['Gross_Margin'],
        mode='markers',
        name='Mid',
        marker= dict(size= 9,
            line= dict(width=1),
            color= 'green',
            opacity= 0.5
           )
    ),
        go.Scatter(
        x=tx_graph.query("Segment == 'High-Value'")['Recency'],
        y=tx_graph.query("Segment == 'High-Value'")['Gross_Margin'],
        mode='markers',
        name='High',
        marker= dict(size= 11,
            line= dict(width=1),
            color= 'red',
            opacity= 0.9
           )
    ),
]

plot_layout = go.Layout(
        yaxis= {'title': "Gross_Margin"},
        xaxis= {'title': "Recency"},
        title='Segments'
    )
fig = go.Figure(data=plot_data, layout=plot_layout)
pyoff.iplot(fig)

#plot frequency vs recency
tx_graph = tx_user.query("Gross_Margin < 50000 and Frequency < 2000")
#tx_graph=tx_user.copy()
plot_data = [
    go.Scatter(
        x=tx_graph.query("Segment == 'Low-Value'")['Recency'],
        y=tx_graph.query("Segment == 'Low-Value'")['Frequency'],
        mode='markers',
        name='Low',
        marker= dict(size= 7,
            line= dict(width=1),
            color= 'blue',
            opacity= 0.8
           )
    ),
        go.Scatter(
        x=tx_graph.query("Segment == 'Mid-Value'")['Recency'],
        y=tx_graph.query("Segment == 'Mid-Value'")['Frequency'],
        mode='markers',
        name='Mid',
        marker= dict(size= 9,
            line= dict(width=1),
            color= 'green',
            opacity= 0.5
           )
    ),
        go.Scatter(
        x=tx_graph.query("Segment == 'High-Value'")['Recency'],
        y=tx_graph.query("Segment == 'High-Value'")['Frequency'],
        mode='markers',
        name='High',
        marker= dict(size= 11,
            line= dict(width=1),
            color= 'red',
            opacity= 0.9
           )
    ),
]

plot_layout = go.Layout(
        yaxis= {'title': "Frequency"},
        xaxis= {'title': "Recency"},
        title='Segments'
    )
fig = go.Figure(data=plot_data, layout=plot_layout)
pyoff.iplot(fig)

#create a dataframe with CustomerID and Invoice Date
tx_day_order = tx_6m[['Customer_Number','Date']]
#convert Invoice Datetime to day
tx_day_order['InvoiceDay'] = tx_6m['Date'].dt.date
tx_day_order = tx_day_order.sort_values(['Customer_Number','Date'])
#drop duplicates
tx_day_order = tx_day_order.drop_duplicates(subset=['Customer_Number','InvoiceDay'],keep='first')

#shifting last 3 purchase dates
tx_day_order['PrevInvoiceDate'] = tx_day_order.groupby('Customer_Number')['InvoiceDay'].shift(1)
tx_day_order['T2InvoiceDate'] = tx_day_order.groupby('Customer_Number')['InvoiceDay'].shift(2)
tx_day_order['T3InvoiceDate'] = tx_day_order.groupby('Customer_Number')['InvoiceDay'].shift(3)

tx_day_order

tx_day_order['DayDiff'] = (tx_day_order['InvoiceDay'] - tx_day_order['PrevInvoiceDate']).dt.days
tx_day_order['DayDiff2'] = (tx_day_order['InvoiceDay'] - tx_day_order['T2InvoiceDate']).dt.days
tx_day_order['DayDiff3'] = (tx_day_order['InvoiceDay'] - tx_day_order['T3InvoiceDate']).dt.days

tx_day_order.head(10)

tx_day_diff = tx_day_order.groupby('Customer_Number').agg({'DayDiff': ['mean','std']}).reset_index()
tx_day_diff.columns = ['Customer_Number', 'DayDiffMean','DayDiffStd']

tx_day_order_last = tx_day_order.drop_duplicates(subset=['Customer_Number'],keep='last')

tx_day_order_last = tx_day_order_last.dropna()
tx_day_order_last = pd.merge(tx_day_order_last, tx_day_diff, on='Customer_Number')
tx_user = pd.merge(tx_user, tx_day_order_last[['Customer_Number','DayDiff','DayDiff2','DayDiff3','DayDiffMean','DayDiffStd']], on='Customer_Number')
#create tx_class as a copy of tx_user before applying get_dummies
tx_class = tx_user.copy()
tx_class = pd.get_dummies(tx_class)

tx_user.NextPurchaseDay.describe()

tx_class['NextPurchaseDayRange'] = 2
tx_class.loc[tx_class.NextPurchaseDay>30,'NextPurchaseDayRange'] = 1
tx_class.loc[tx_class.NextPurchaseDay>80,'NextPurchaseDayRange'] = 0

corr = tx_class[tx_class.columns].corr()
plt.figure(figsize = (18,10))-
sns.heatmap(corr, annot = True, linewidths=0.2, fmt=".2f")

tx_class = tx_class.drop('NextPurchaseDay',axis=1)
X, y = tx_class.drop('NextPurchaseDayRange',axis=1), tx_class.NextPurchaseDayRange

#train & test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=44)
#create an array of models
models = []
models.append(("LR",LogisticRegression()))
models.append(("NB",GaussianNB()))
models.append(("RF",RandomForestClassifier()))
models.append(("SVC",SVC()))
models.append(("Dtree",DecisionTreeClassifier()))
models.append(("XGB",xgb.XGBClassifier()))
models.append(("KNN",KNeighborsClassifier()))

#measure the accuracy 
for name,model in models:
    kfold = KFold(n_splits=2, random_state=22)
    cv_result = cross_val_score(model,X_train,y_train, cv = kfold,scoring = "accuracy")
    print(name, cv_result)

xgb_model = xgb.XGBClassifier().fit(X_train, y_train)
print('Accuracy of XGB classifier on training set: {:.2f}'
       .format(xgb_model.score(X_train, y_train)))
print('Accuracy of XGB classifier on test set: {:.2f}'
       .format(xgb_model.score(X_test[X_train.columns], y_test)))

from sklearn.model_selection import GridSearchCV
param_test1 = {
 'max_depth':range(3,10,2),
 'min_child_weight':range(1,6,2)
}
gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier(), 
param_grid = param_test1, scoring='accuracy',n_jobs=-1,iid=False, cv=2)
gsearch1.fit(X_train,y_train)
gsearch1.best_params_, gsearch1.best_score_

xgb_model = xgb.XGBClassifier(max_depth=3,min_child_weight=5).fit(X_train, y_train)
print('Accuracy of XGB classifier on training set: {:.2f}'
       .format(xgb_model.score(X_train, y_train)))
print('Accuracy of XGB classifier on test set: {:.2f}'
       .format(xgb_model.score(X_test[X_train.columns], y_test)))

##Time series based on LSTM  
#represent month in date field as its first day\
sales_dataFrame=store_df.copy()
sales_dataFrame['Date'] = sales_dataFrame['Date'].dt.year.astype('str') + '-' + sales_dataFrame['Date'].dt.month.astype('str') + '-01'
sales_dataFrame['Date'] = pd.to_datetime(sales_dataFrame['Date'])

sales_dataFrame = sales_dataFrame.groupby('Date').Net_Sales_Units.sum().reset_index()

sales_dataFrame.head()

#plot monthly sales
plt.figure(figsize=(18,4))
plt.plot(sales_dataFrame['Date'],sales_dataFrame['Net_Sales_Units'])
plt.title("Montly Sales")
plt.savefig('/content/drive/My Drive/DataScience/FinalProject/'+storeName+'/LSTM_MonthlySales.png', bbox_inches = "tight")

#create a new dataframe to model the difference
sales_differenceDf = sales_dataFrame.copy()

#add previous sales to the next row
sales_differenceDf['prev_sales'] = sales_differenceDf['Net_Sales_Units'].shift(1)

sales_differenceDf.head()

#drop the null values and calculate the difference
sales_differenceDf = sales_differenceDf.dropna()

sales_differenceDf['diff'] = (sales_differenceDf['Net_Sales_Units'] - sales_differenceDf['prev_sales'])

sales_differenceDf.head(10)

#plot sales diff
plt.figure(figsize=(18,4))
plt.plot(sales_differenceDf['Date'],sales_differenceDf['diff'])
plt.title("Sales Difference monthly basis -Stationary Data")
plt.savefig('/content/drive/My Drive/DataScience/FinalProject/'+storeName+'/LSTM_MonthlySales_stationary.png', bbox_inches = "tight")

#create new dataframe from transformation from time series to supervised
supervised_dataframe = sales_differenceDf.drop(['prev_sales'],axis=1)

#adding lags
for inc in range(1,13):
    field_name = 'lag_' + str(inc)
    supervised_dataframe[field_name] = supervised_dataframe['diff'].shift(inc)

#Drop months for getting non nan values looking at the past
supervised_dataframe=supervised_dataframe[12:]

supervised_dataframe

import statsmodels.formula.api as smf 
# Regression formula
ols_model = smf.ols(formula='diff ~ lag_1', data=supervised_dataframe)
# Fit 
ols_model_fit = ols_model.fit()
#adjusted r-squared
regression_adj_rsq = ols_model_fit.rsquared_adj
print(regression_adj_rsq)

import statsmodels.formula.api as smf 
# Define the regression formula
ols_model = smf.ols(formula='diff ~ lag_1 + lag_2 + lag_3 + lag_4 + lag_5 + lag_6 + lag_7 + lag_8 + lag_9 + lag_10+lag_11+lag_12 ', data=supervised_dataframe)

# Fit 
ols_model_fit = ols_model.fit()
# Adjusted r-squared
regression_adj_rsq = ols_model_fit.rsquared_adj
print(regression_adj_rsq)

from sklearn.preprocessing import MinMaxScaler
df_model = supervised_dataframe.drop(['Net_Sales_Units','Date'],axis=1)

#split train and test set
train_set, test_set = df_model[0:-6].values, df_model[-6:].values

#apply Min Max Scaler
scaler = MinMaxScaler(feature_range=(-1, 1))
scaler = scaler.fit(train_set)
# reshape training set
train_set = train_set.reshape(train_set.shape[0], train_set.shape[1])
train_set_scaled = scaler.transform(train_set)

# reshape test set
test_set = test_set.reshape(test_set.shape[0], test_set.shape[1])
test_set_scaled = scaler.transform(test_set)

X_train, y_train = train_set_scaled[:, 1:], train_set_scaled[:, 0:1]
X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])

X_test, y_test = test_set_scaled[:, 1:], test_set_scaled[:, 0:1]
X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])

model = Sequential()
model.add(LSTM(4, batch_input_shape=(1, X_train.shape[1], X_train.shape[2]), stateful=True))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
model.fit(X_train, y_train, nb_epoch=100, batch_size=1, verbose=1, shuffle=False)

y_pred = model.predict(X_test,batch_size=1)

#reshape y_pred
y_pred = y_pred.reshape(y_pred.shape[0], 1, y_pred.shape[1])

#rebuild test set for inverse transform
pred_test_set = []
for index in range(0,len(y_pred)):
    print(np.concatenate([y_pred[index],X_test[index]],axis=1))
    pred_test_set.append(np.concatenate([y_pred[index],X_test[index]],axis=1))

#reshape pred_test_set
pred_test_set = np.array(pred_test_set)
pred_test_set = pred_test_set.reshape(pred_test_set.shape[0], pred_test_set.shape[2])

#inverse transform
pred_test_set_inverted = scaler.inverse_transform(pred_test_set)

#create dataframe that shows the predicted sales
result_list = []
sales_dates = list(sales_dataFrame[-7:].Date)
act_sales = list(sales_dataFrame[-7:].Net_Sales_Units)
for index in range(0,len(pred_test_set_inverted)):
    result_dict = {}
    result_dict['Date'] = sales_dates[index+1]
    result_dict['pred_value'] = int(pred_test_set_inverted[index][0] + act_sales[index])
    result_list.append(result_dict)
df_result = pd.DataFrame(result_list)

#merge with actual sales dataframe
df_sales_pred = pd.merge(sales_dataFrame,df_result,on='Date',how='left')

dfpred_netsale=sales_dataFrame[-6:]
# print(dfpred_netsale, 
print(df_result)
df_result.index
df_result

ax = df_result.plot(x='Date',y='pred_value',label='observed')
sales_dataFrame.plot(x='Date',y='Net_Sales_Units',ax=ax, label='Validate', alpha=.7, figsize=(14, 7),linewidth=3)
plt.legend()
plt.savefig('/content/drive/My Drive/DataScience/FinalProject/'+storeName+'/LSTM_Validation.png', bbox_inches = "tight")
plt.show()

#Drop columns with redundant info for time series analysis
lst=['Store_Name','Item_Description','Department_Name','Fineline_Name','Clerk','Line_#','Class_Name','Item_was_Scanned']
workspace_df.drop(columns=lst,inplace=True)

#df['df column_name'].apply(lambda x: 'value if condition is met' if x condition else 'value if condition is not met')
partial_df['Holiday Season']='No season'
partial_df.loc[(partial_df["Month"] ==1) & (partial_df["Day"] >= 7) &(partial_df['Day']<=14), "Holiday Season"]='Martin Luther King Jr'
partial_df.loc[(partial_df["Month"] ==2) & (partial_df["Day"] >= 19) &(partial_df['Day']<=24), "Holiday Season"]='Presidents Day '
partial_df.loc[(partial_df["Month"] ==3) & (partial_df["Day"] >= 14) &(partial_df['Day']<=21), "Holiday Season"]='St. Patricks Day'
partial_df.loc[(partial_df["Month"] ==1) & (partial_df["Day"] >= 1) &(partial_df['Day']<=7), "Holiday Season"]='NewYear'
partial_df.loc[(partial_df["Month"] ==12) & (partial_df["Day"] >= 20) &(partial_df['Day']<=31), "Holiday Season"]='Christmas'
partial_df.loc[(partial_df["Month"] ==11) & (partial_df["Day"] >= 1) &(partial_df['Day']<=30), "Holiday Season"]='ThanksGiving'

backup_df=partial_df.copy()

plt.figure(figsize=(18,6))
plt.plot(partial_df['Holiday Season'],partial_df['Net Sales Units'],color='r')
plt.bar(partial_df['Holiday Season'],partial_df['Net Sales Units'],color='c')
plt.xticks(rotation=90)
plt.tight_layout
plt.title('Net Sales Units vs Major Holiday Seasons')
plt.ylabel('Net Sale Units')
plt.xlabel('Holiday Seasons')
#plt.tight_layout()
plt.savefig('/content/drive/My Drive/DataScience/FinalProject/NetSaleUnitsVsMajorHolidaySeasons.png', bbox_inches = "tight")
plt.show()

#Cleaning data
partial_df=backup_df.copy()
partial_df['Net Sales']=partial_df['Net Sales'].astype(str)
partial_df['Net Sales']=partial_df['Net Sales'].str.replace(',', '')
partial_df=partial_df[partial_df['Store #'] != 'Store #']
partial_df["Net Sales"] = pd.to_numeric(partial_df["Net Sales"])

thanksGiving_df=partial_df[partial_df['Holiday Season']=='ThanksGiving']
store_thanksGiv_df=thanksGiving_df.groupby('Store Name').sum().reset_index()
store_thanksGiv_df=store_thanksGiv_df[store_thanksGiv_df['Store Name']!='13168 BOHEMIA WAREHOUSE']
plt.figure(figsize=(18,6))
plt.plot(store_thanksGiv_df['Store Name'],store_thanksGiv_df['Net Sales'],color='r')
plt.bar(store_thanksGiv_df['Store Name'],store_thanksGiv_df['Net Sales'],color='c')
plt.xticks(rotation=90)
plt.tight_layout
plt.title('Store Name vs Net Sales(Revenue) for ThanksGiving Holiday')
plt.ylabel('Net Sales')
plt.xlabel('Store Name')
plt.savefig('/content/drive/My Drive/DataScience/FinalProject/StoreNameVsNetSales.png', bbox_inches = "tight")
plt.show()

maxRevenueGeneratorStoreInThanksGiving_df=thanksGiving_df[thanksGiving_df['Store Name']=='14252 ISLAND PARK']
StoreNamevsDepartmentName_df=maxRevenueGeneratorStoreInThanksGiving_df.groupby('Department Name').sum().reset_index()    
removeNegativeFromDepartmentNetSales_df_df=StoreNamevsDepartmentName_df[StoreNamevsDepartmentName_df['Net Sales']>=0]
removeNegativeFromDepartmentNetSales_df_df
plt.figure(figsize=(18,6))
plt.plot(removeNegativeFromDepartmentNetSales_df_df['Department Name'],removeNegativeFromDepartmentNetSales_df_df['Net Sales'],color='r')
plt.bar(removeNegativeFromDepartmentNetSales_df_df['Department Name'],removeNegativeFromDepartmentNetSales_df_df['Net Sales'],color='c')
plt.xticks(rotation=90)
plt.tight_layout
plt.title('Department Name vs Net Sales(Revenue) for 14252 ISLAND PARK store in  ThanksGiving Holiday')
plt.ylabel('Net Sales')
plt.xlabel('Department Name')
plt.savefig('/content/drive/My Drive/DataScience/FinalProject/DepartmentNameVsNetSales.png', bbox_inches = "tight")
plt.show()

import matplotlib.pyplot as plt
full_df.groupby('Department_Name')['Department_Name'].count().plot(kind='bar',title='deptname vs count',figsize=(20,10))
plt.show()